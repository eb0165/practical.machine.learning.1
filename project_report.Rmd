---
title: "Machine Learning Project"
author: "Edward Bruggemann"
date: "June 13, 2016"
output: 
  html_document: 
    keep_md: yes
---

### Summary
The data for this project comprises accelerometer output from the belt, forearm, arm, and dumbell of 6 participants.  The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  The goal is to predict the manner in which the participants did the exercise from the accelerometer output.  A training dataset and a testing dataset were provided.  The caret package was used to build and test models, and to make predictions.  A random forest model successfully predicted the testing classifications.

```{r, cashe=TRUE}
## load caret
library(caret)
```

### Datasets
I downloaded the datasets using the URL's provided in the instructions.

```{r, cache=TRUE}
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train1 <- read.csv(url1)
test1 <- read.csv(url2)
```

The training dataset comprises 19,622 rows x 160 columns and the testing dataset comprises 20 rows by 160 columns. There is only one column name difference.  Column "classe" in the training data contains the known classifications as a factor of five levels: A, B, C, D, and E.  In the testing dataset this column is replaced with "problem_id", numbered 1 - 20, which corresponds to the numbered quiz questions for submission.

```{r}
names(train1)[!names(train1)==names(test1)]
names(test1)[!names(train1)==names(test1)]
```

###Dataset Cleanup
Both datasets contain many columns that are entirely or almost entirely NA; all other columns are complete.  I included for further analysis only columns that were complete in both datasets.  Train2 and Test2 both contain 60 columns, of which 59 are potential predictors and 1 is the response variable.

```{r}
keepcols <- colSums(is.na(train1))==0 & colSums(is.na(test1))==0
train2 <- train1[, keepcols]
test2 <- test1[, keepcols]
```

Inspection of column names and data revealed that columns 1-7 could be excluded as potential predictors.  Train3 and Test3 both 53 columns, of which 52 are potentail predictors and 1 is the response variable.

```{r}
names(train2)[1:7]
train3 <- train2[, -(1:7)]
test3 <- test2[, -(1:7)]
```

###Dataset Processing
A quick search for variables with low variance or no variance revealed none.

```{r}
low.var <- nearZeroVar(train3[, -53], saveMetrics=TRUE)
sum(low.var$zeroVar)
sum(low.var$nzv)
```

A quick search for variables with high correlation revealed a handful, which I excluded from further analysis.  The cutoff of 0.80 is arbitrary.   Train4 and Test4 both contain 40 columns, of which 39 are potential predictors and 1 is the response variable.

```{r}
cormat <- cor(train3[, -53])
highcorr <- findCorrelation(cormat, cutoff=0.80)
train4 <- train3[,-highcorr]
test4 <- test3[,-highcorr]
```

###Cross Validation
I partitioned the training data into train4A for training the model and train4B for testing and estimating out-of-sample accuracy before applying the model to the testing dataset.

```{r}
set.seed(4674833)
partindex <- createDataPartition(train4$classe, p=0.80, list=FALSE)
train4A <- train4[partindex,]
train4B <- train4[-partindex,]
```

### Building the Model
I selected a random forest model because in the lectures it was asserted several times that this method is often the top performer.  I used default parameter settings in all cases, except for trainControl(method="cv"), which appeared to be faster than method="boot" on my machine.

```{r, cache=TRUE}
## very slow
t1 <- Sys.time()
set.seed(9892225)
model <- train(classe~., data=train4A, method="rf",
                trControl=trainControl(method = "cv"))
t2 <- Sys.time()
t2-t1
```

###Predictions from the Model
I used this model to predict on Train4B.  Out-of-sample accuracy was 0.9934.  Good enough to proceed to the testing dataset.

```{r}
predictB <- predict(model, newdata=train4B)
conmatB <- confusionMatrix(predictB, train4B$classe)
print(conmatB)
```

I used this model to predict the testing set and submitted the predictions for grading. The result was 100% accurate.

```{r}
predict.test <- predict(model, newdata=test4)
print(predict.test)
```
